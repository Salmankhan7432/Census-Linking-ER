{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import operator\n",
    "import DWM10_Parms\n",
    "def tokenizeInput():\n",
    "    #***********Inner Function*******************************\n",
    "    #Replace delimiter with blanks, then compress token by replacing non-word characters with null\n",
    "    def tokenizerCompress(string):\n",
    "        string = string.upper()\n",
    "        string = string.replace(delimiter,' ')\n",
    "        tokenList = re.split('[\\s]+',string)\n",
    "        newList = []\n",
    "        for token in tokenList:\n",
    "            newToken = re.sub('[\\W]+','',token)\n",
    "            if len(newToken)>0:\n",
    "                newList.append(newToken)\n",
    "        return newList\n",
    "    #***********Inner Function*******************************\n",
    "    #Replace all non-words characters with blanks, then split on blanks\n",
    "    def tokenizerSplitter(string):\n",
    "        string = string.upper()\n",
    "        string = re.sub('[\\W]+',' ',string)\n",
    "        tokenList = re.split('[\\s]+',string)\n",
    "        newList = []\n",
    "        for token in tokenList:\n",
    "            if len(token)>0:\n",
    "                newList.append(token)\n",
    "        return newList\n",
    "    #***********Outer Main Function*******************************\n",
    "    # Start of Main Tokenizer Function\n",
    "    logFile = DWM10_Parms.logFile\n",
    "    print('\\n>> Starting DWM14')\n",
    "    print('\\n>> Starting DWM14', file=logFile)\n",
    "    inputFileName = DWM10_Parms.inputFileName\n",
    "    refDict = {}\n",
    "    print('Input Reference File Name =',inputFileName)\n",
    "    print('Input Reference File Name =',inputFileName, file=logFile)\n",
    "    hasHeader = DWM10_Parms.hasHeader\n",
    "    print('Input File has Header Records =', hasHeader)\n",
    "    print('Input File has Header Records =', hasHeader, file=logFile)\n",
    "    delimiter = DWM10_Parms.delimiter\n",
    "    print('Input File Delimiter =',delimiter)\n",
    "    print('Input File Delimiter =',delimiter, file=logFile)\n",
    "    tokenizerType = DWM10_Parms.tokenizerType\n",
    "    print('Tokenizer Function Type =',tokenizerType)\n",
    "    print('Tokenizer Function Type =',tokenizerType, file=logFile)\n",
    "    removeDuplicateTokens = DWM10_Parms.removeDuplicateTokens\n",
    "    print('Remove Duplicate Reference Tokens =',removeDuplicateTokens)\n",
    "    print('Remove Duplicate Reference Tokens =',removeDuplicateTokens, file=logFile)\n",
    "    goodType = False\n",
    "    if tokenizerType=='Splitter':\n",
    "        tokenizerFunction = tokenizerSplitter\n",
    "        goodType = True\n",
    "    if tokenizerType=='Compress':\n",
    "        tokenizerFunction = tokenizerCompress\n",
    "        goodType = True\n",
    "    if goodType == False:\n",
    "        print('**Error: Invalid Parameter value for tokenizerType ',tokenizerType)\n",
    "        sys.exit()\n",
    "    # Read input file and build reference dictionary (refDict)\n",
    "    inputFile= open(inputFileName,'r')\n",
    "    refCnt = 0\n",
    "    # skip header record\n",
    "    if hasHeader:\n",
    "        line = inputFile.readline()\n",
    "    line = inputFile.readline()\n",
    "    tokenCnt = 0\n",
    "    tokensOut = 0\n",
    "    tokenFreqCount = []\n",
    "    while line !='':\n",
    "        refCnt +=1\n",
    "        line = line.strip()\n",
    "        # assume first token is the reference identifier (refID)\n",
    "        firstDelimiter = line.find(delimiter)\n",
    "        refID = line[0:firstDelimiter]\n",
    "        body = line[firstDelimiter+1:]\n",
    "        tokenList = tokenizerFunction(body)\n",
    "        tokenCnt = tokenCnt + len(tokenList)\n",
    "        if removeDuplicateTokens:\n",
    "            tokenList = list(dict.fromkeys(tokenList))\n",
    "        tokensOut = tokensOut + len(tokenList)\n",
    "        refDict[refID] = tokenList          \n",
    "        line = inputFile.readline()\n",
    "    inputFile.close()\n",
    "    print('Total References Read=',refCnt)\n",
    "    print('Total References Read=',refCnt, file=logFile)\n",
    "    print('Total Tokens Found =',tokenCnt)\n",
    "    print('Total Tokens Found =',tokenCnt, file=logFile)\n",
    "    return refDict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
